{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of raw blood donation data for use in SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import pickle\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading original data files\n",
    "\n",
    "Source data: \n",
    "- all donations 2008 - 2020     - alledonaties_2008_2020.csv\n",
    "- donations in 2021            - 2021Donatiesplus.txt\n",
    "\n",
    "Sex and date of birth are already included in the donations file in this version, so the donor file is not needed. Matching columns in both data sets are selected and data is combined into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "donations_raw = pd.read_csv(data_path+'alledonaties_2008_2020.csv', low_memory=False)\n",
    "don_2021_raw = pd.read_csv(data_path+'2021Donatiesplus.txt', sep='\\t', low_memory=False)\n",
    "file = open(data_path+'donatiesoortcodes.txt', 'r')\n",
    "contents = file.read()\n",
    "donatiesoort_dict = ast.literal_eval(contents)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "don_2008_2020 = donations_raw.copy()\n",
    "don_2021 = don_2021_raw.copy()\n",
    "\n",
    "# Only keep donations with permission to use in research\n",
    "don_2008_2020 = don_2008_2020.loc[(don_2008_2020['WOtoestemming'] == 'Ja') | (don_2008_2020['ToestemmingWO'] == 'Ja'), ]\n",
    "don_2021 = don_2021.loc[don_2021['ToestemmingWO'] == 'Ja', ]\n",
    "\n",
    "# Select relevant columns\n",
    "don_2008_2020 = don_2008_2020[['KeyID', 'Geslacht', 'Geboortedatum', 'Einnummer', \n",
    "                       'Donatiedatum', 'Donatie_Tijd_Start', 'Donatiecentrumcode',\n",
    "                       'Donatiesoortcode', 'AfgenomenVolume',\n",
    "                       'hb', 'HbGoedgekeurd', 'Ferritine']].rename(columns={'hb':'Hb'})\n",
    "don_2021 = don_2021[['KeyID', 'Geslacht', 'Geboortedatum', 'EINnummer', \n",
    "                       'Donatiedatum', 'Donatie_Tijd_Start', 'Donatiecentrumcode',\n",
    "                       'Donatiesoortcode', 'AfgenomenVolume',\n",
    "                       'Hb', 'HbGoedgekeurd', 'Ferritine']].rename(columns={'EINnummer':'Einnummer'})\n",
    "\n",
    "# Map column values for 'Donatiesoortcode' in donations to those of don_2021\n",
    "don_2008_2020['Donatiesoortcode'].replace(donatiesoort_dict, inplace=True)\n",
    "\n",
    "# Combine into one dataframe\n",
    "donations = pd.concat([don_2008_2020, don_2021])\n",
    "donations = donations.rename(columns = {\n",
    "                            'Geslacht':'Sex', 'Geboortedatum':'DoB', 'Einnummer':'EIN',\n",
    "                            'Donatiedatum':'Date', 'Donatie_Tijd_Start':'Time', \n",
    "                            'Donatiecentrumcode':'Center', 'Donatiesoortcode':'DonType', \n",
    "                            'AfgenomenVolume':'Volume', 'HbGoedgekeurd':'HbOK', 'Ferritine':'Ferritin'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of donors: 931533 \n",
      "Number of donors with only whole-blood donations: 756007 \n",
      "Number of donors with only whole-blood donations, with donor intake: 485314\n"
     ]
    }
   ],
   "source": [
    "all_dontypes = np.unique(donations['DonType'])\n",
    "oth_dontypes = all_dontypes[(all_dontypes != 'V') & \n",
    "                            (all_dontypes != 'N')]\n",
    "donors_oth_dontypes = donations.loc[donations['DonType'].isin(oth_dontypes), 'KeyID']\n",
    "\n",
    "d_tot = np.unique(donations['KeyID'])\n",
    "d_wb = np.unique(donations.loc[~donations['KeyID'].isin(donors_oth_dontypes), 'KeyID'])\n",
    "d_wbn = np.unique(donations.loc[(donations['KeyID'].isin(d_wb)) & (donations['DonType'] == 'N'), 'KeyID'])\n",
    "\n",
    "print('Total number of donors:', len(d_tot),\n",
    "      '\\nNumber of donors with only whole-blood donations:', len(d_wb),\n",
    "      '\\nNumber of donors with only whole-blood donations, with donor intake:', len(d_wbn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting donors and donations:\n",
    "\n",
    "- Keep donors with only whole-blood donations\n",
    "- Drop donors without donor intake in dataset\n",
    "- Drop rows where Hb is not measured (Hb == 'niet bepaald')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = donations.loc[donations['KeyID'].isin(d_wbn), ].copy()\n",
    "data = data.loc[(data['HbOK'] != 'niet gekeurd') & (data['Hb'] != 'niet bepaald'), ]\n",
    "data['Hb'] = pd.to_numeric(data['Hb'], errors='coerce')\n",
    "data = data.loc[data['Sex'].isin(['M','F']), ]\n",
    "data = data.dropna(axis=0, subset=['Hb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datestr_to_date(datestr):\n",
    "    month, day, year = datestr.split('/')\n",
    "    dob = year + '/' + month.zfill(2) + '/' + day.zfill(2)\n",
    "    return dob\n",
    "\n",
    "def timestr_to_float(timestr):\n",
    "    hour, minute = timestr.split(':')\n",
    "    time = int(hour) + int(minute) / 60\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DoB'] = data['DoB'].apply(datestr_to_date)\n",
    "data['Date'] = data['Date'].apply(datestr_to_date)\n",
    "data['Time'] = data['Time'].apply(timestr_to_float)\n",
    "data['DoB'] = pd.to_datetime(data['DoB'])\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['HbOK'].replace({'afgekeurd':'0', 'goedgekeurd':'1'}, inplace=True)\n",
    "data[['Volume', 'HbOK', 'Ferritin']] = data[['Volume', 'HbOK', 'Ferritin']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "data = data.loc[data['HbOK'].isin([0, 1]), ]\n",
    "data = data.dropna(axis=0, subset=['KeyID', 'Sex', 'DoB', 'EIN', 'Date', 'Time', 'DonType', 'Hb', 'HbOK'])\n",
    "\n",
    "data.to_pickle(data_path+'data_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more variables\n",
    "\n",
    "- Age at day of donation \n",
    "- Month of visit \n",
    "- Year of visit\n",
    "- Number of previous visits in past 2 years\n",
    "- Previous ferritin level\n",
    "- Days since previous ferritin level\n",
    "- Previous Hb (for up to 5 previous visits)\n",
    "- Days since previous Hb (for up to 5 previous visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(data_path+'data_clean.pkl')\n",
    "\n",
    "data['Age'] = (data['Date'] - data['DoB']) / pd.Timedelta('365.25d')\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Vol_over_250'] = data['Volume'] > 250\n",
    "data = data.sort_values(['KeyID', 'Date']).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prev_hb_time(df, number):\n",
    "    colnames = ['HbPrev'+str(number), 'TimetoPrev'+str(number)]\n",
    "    df[colnames[0]] = df['Hb'].shift(number)\n",
    "    df[colnames[1]] = (df['Date'] - df['Date'].shift(number)) / pd.Timedelta('1 day') \n",
    "    return(df)\n",
    "\n",
    "def add_numdon_inner(df):\n",
    "    df['Num_Don'] = df['Vol_over_250'].rolling('730d', closed='left').sum()\n",
    "    return(df)\n",
    "\n",
    "def add_numdon(df):\n",
    "    df['index'] = df.index\n",
    "    df = df.set_index('Date', drop=False)\n",
    "    df = df.groupby('KeyID').apply(add_numdon_inner)\n",
    "    df = df.set_index('index')\n",
    "    return(df)\n",
    "\n",
    "def add_last_ferritin(df):\n",
    "    fers = df.loc[df['Ferritin'].notnull(), ['Date', 'Ferritin']]\n",
    "    if (fers.shape[0] == 0):\n",
    "        df['Last_Fer'] = np.NaN\n",
    "        df['Last_Fer_Date'] = np.NaN\n",
    "    else:\n",
    "        df = pd.merge_asof(df, fers, left_index=True, right_index=True, allow_exact_matches=False, direction='backward', suffixes=['','_fer'])\n",
    "        df = df.rename(columns={'Ferritin_fer':'Last_Fer',\n",
    "                                'Date_fer':'Last_Fer_Date'})\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 18:32:20.154873 Starting\n",
      "2022-07-07 18:32:20.642739 Dropped pre-2015\n",
      "2022-07-07 18:49:51.315182 NumDon done\n",
      "2022-07-07 19:11:02.106720 Hb1 done\n",
      "2022-07-07 19:34:27.382617 Hb2 done\n",
      "2022-07-07 19:54:32.050661 Hb3 done\n",
      "2022-07-07 20:16:21.923161 Hb4 done\n",
      "2022-07-07 20:40:45.329132 Hb5 done\n",
      "2022-07-07 21:20:44.572392 Ferritin done\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now(), 'Starting')\n",
    "df = data.loc[data.Year > 2014, ].copy()\n",
    "print(datetime.datetime.now(), 'Dropped pre-2015')\n",
    "df = add_numdon(df)\n",
    "print(datetime.datetime.now(), 'NumDon done')\n",
    "df_1 = df.groupby('KeyID').apply(add_prev_hb_time, number=1)\n",
    "print(datetime.datetime.now(), 'Hb1 done')\n",
    "df_2 = df_1.groupby('KeyID').apply(add_prev_hb_time, number=2)\n",
    "print(datetime.datetime.now(), 'Hb2 done')\n",
    "df_3 = df_2.groupby('KeyID').apply(add_prev_hb_time, number=3)\n",
    "print(datetime.datetime.now(), 'Hb3 done')\n",
    "df_4 = df_3.groupby('KeyID').apply(add_prev_hb_time, number=4)\n",
    "print(datetime.datetime.now(), 'Hb4 done')\n",
    "df_5 = df_4.groupby('KeyID').apply(add_prev_hb_time, number=5)\n",
    "print(datetime.datetime.now(), 'Hb5 done')\n",
    "df_5.head()\n",
    "\n",
    "df_5f = df_5.groupby('KeyID').apply(add_last_ferritin)\n",
    "df_5f['TimetoFer'] = (df_5f['Date'] - df_5f['Last_Fer_Date']) / pd.Timedelta('1d')\n",
    "print(datetime.datetime.now(), 'Ferritin done')\n",
    "\n",
    "df_5f.to_csv(data_path+'df_allvars.csv', index=False)\n",
    "df_5f.to_pickle(data_path+'df_allvars.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal distributions of variables per SVM\n",
    "\n",
    "Age, time, month, ferritin, timetofer, hbprevn, timetoprevn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled based on training data\n",
    "\n",
    "We need to scale all explanatory variables before doing anything with the SVM. We use the StandardScaler option in the sk-learn package, which makes all variables have a mean of zero and variance of one. We save the scalers for later use when we change time-related variables. Scalers are fitted using only the training data and then used to transform both training and test data.\n",
    "\n",
    "Test data will be the last year of donations (1 January 2021 - 31 December 2021) and training data everything before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path+'df_allvars.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['KeyID', 'Year', 'Sex', 'Time', 'Age', 'Month', 'Num_Don', 'Last_Fer', 'TimetoFer']\n",
    "\n",
    "for n in range(1, 6):\n",
    "    var.extend(['HbPrev'+str(n), 'TimetoPrev'+str(n)])\n",
    "var.append('HbOK')\n",
    "\n",
    "train_men = df.loc[(df.Sex == 'M') & (df.Year <= 2020), var]\n",
    "train_men = train_men[train_men.columns[3:]]\n",
    "train_women = df.loc[(df.Sex == 'F') & (df.Year <= 2020), var]\n",
    "train_women = train_women[train_women.columns[3:]]\n",
    "\n",
    "test_men = df.loc[(df.Sex == 'M') & (df.Year == 2021), var]\n",
    "test_men = test_men[test_men.columns[3:]]\n",
    "test_women = df.loc[(df.Sex == 'F') & (df.Year == 2021), var]\n",
    "test_women = test_women[test_women.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nback in range(1, 6):\n",
    "    var = ['Time', 'Age', 'Month', 'Num_Don', 'Last_Fer', 'TimetoFer']\n",
    "    for n in range(1, nback+1):\n",
    "        var.extend(['HbPrev'+str(n), 'TimetoPrev'+str(n)])\n",
    "    var.append('HbOK')\n",
    "    \n",
    "    train_men_sub = train_men[var].dropna()\n",
    "    train_women_sub = train_women[var].dropna()\n",
    "    test_men_sub = test_men[var].dropna()\n",
    "    test_women_sub = test_women[var].dropna()\n",
    "    \n",
    "    scaler_men = StandardScaler()\n",
    "    scaler_women = StandardScaler()\n",
    "    scaler_men.fit(train_men_sub[train_men_sub.columns[:-1]])\n",
    "    scaler_women.fit(train_women_sub[train_men_sub.columns[:-1]])\n",
    "    \n",
    "    train_men_sub[train_men_sub.columns[:-1]] = scaler_men.transform(train_men_sub[train_men_sub.columns[:-1]])\n",
    "    train_women_sub[train_women_sub.columns[:-1]] = scaler_women.transform(train_women_sub[train_women_sub.columns[:-1]])\n",
    "    test_men_sub[test_men_sub.columns[:-1]] = scaler_men.transform(test_men_sub[test_men_sub.columns[:-1]])\n",
    "    test_women_sub[test_women_sub.columns[:-1]] = scaler_women.transform(test_women_sub[test_women_sub.columns[:-1]])\n",
    "    \n",
    "    pickle.dump(scaler_men, open('../results/scalers/men_'+str(nback)+'.pkl', 'wb'))\n",
    "    pickle.dump(scaler_women, open('../results/scalers/women_'+str(nback)+'.pkl', 'wb'))\n",
    "    \n",
    "    train_men_sub.to_pickle(data_path+'scaled/men_'+str(nback)+'_train.pkl')\n",
    "    train_women_sub.to_pickle(data_path+'scaled/women_'+str(nback)+'_train.pkl')\n",
    "    test_men_sub.to_pickle(data_path+'scaled/men_'+str(nback)+'_test.pkl')\n",
    "    test_women_sub.to_pickle(data_path+'scaled/women_'+str(nback)+'_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
